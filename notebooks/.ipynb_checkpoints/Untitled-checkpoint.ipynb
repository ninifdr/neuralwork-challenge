{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba236903",
   "metadata": {},
   "source": [
    "## Tarea NeuralWorks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d1eb2",
   "metadata": {},
   "source": [
    "### 1. Procesos automatizados para ingerir y almacenar los datos bajo demanda <br>\n",
    "a. Los viajes que son similares en términos de origen, destino y hora del día deben agruparse. Describa el enfoque que utilizó para agregar viajes similares.\n",
    "\n",
    "Para esta pregunta se hizo el código que se encuentra en la ruta src/etl.py <br>\n",
    "El código se desarrolló en Python, utilizando SQlite3 como base de datos. En el archivo database/init.py se puede ver la estructura de las dos tablas que utilicé para la solución <br>\n",
    "Para crear la base de datos se debe ejecutar el comando python3 init.py en la consola, dentro de la carpeta database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "69709cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"python3 ../database/init.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f17d5",
   "metadata": {},
   "source": [
    "Se asume que el CSV con la data se deja en la carpeta data/trips.csv\n",
    "<br>\n",
    "Para almacenar viajes que son similares en términos de origen, destino y hora del día se decidió aproximar las latitudes y longitudes a una cantidad menor de decimales. Esta cantidad se puede controlar con el parámetro de la línea 9.<br>\n",
    "Desarrollé el código de manera que se pueda ejecutar muchas veces (insertar múltiples datos más de una vez)\n",
    "<br>\n",
    "<br>\n",
    "Algunos comentarios importantes que quiero hacer sobre mi código:<br>\n",
    "En la línea 86 a la 120 ejecuto tanto una query de inserción y otra de actualización de datos, donde además utilizo una tabla temporal. Esta no es la solución que me hubiese gustado, sin embargo SQLite3 no contempla el comando MERGE (como en SQL Server, por ejemplo), con el que podría haber hecho ambas funciones al mismo tiempo y, además, tampoco se puede updatear una tabla desde un SELECT, y por lo tanto tuve que usar una tabla temporal.<br>Por simplicidad utilicé SQLite3, pero el código queda menos elegante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4769e5",
   "metadata": {},
   "source": [
    "### 2. Un servicio que es capaz de proporcionar la siguiente funcionalidad:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9acc3fb",
   "metadata": {},
   "source": [
    "Para desarrollar el servicio se creo una api en flask. El código está en src/api.py<br>\n",
    "Para levantarla hay que ejecutar:<br>\n",
    "export FLASK_APP=api.py<br>\n",
    "flask run<br>\n",
    "Tiene dos endpoints:<br>\n",
    "1. /trips: Recibe la region y latitudes máximas y mínimas y retorna el promedio de cantidad de viajes.\n",
    "2. /data_status: Retorna el estado de ingesta de datos de la base. En caso de que se estén insertando datos retorna: \"Data being ingested since\" y la hora en la que se empezó a ingestar. En caso de que no, retorna \"No data being ingested\"\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50962c83",
   "metadata": {},
   "source": [
    "Probemos con comandos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d8d1b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e093200",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = \"http://127.0.0.1:5000\" #o la que entregue la consola al ejecutar flask run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ecd2c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"avg_weekly_trips\":0.057654169673924546,\"status\":\"200\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_endpoint = \"/trips\"\n",
    "trips_response = requests.get(\"{}{}\".format(api_url,trips_endpoint), params={\"region\": \"Turin\", \n",
    "                                   \"max_lat\": 100.0, \n",
    "                                   \"min_lat\": -100.0,\n",
    "                                  \"max_lng\": 100.0,\n",
    "                                  \"min_lng\": -100.0})\n",
    "print(trips_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bfd537bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Data\":\"No data being ingested\",\"Status\":\"200\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_endpoint = \"/data_status\"\n",
    "data_status_response = requests.get(\"{}{}\".format(api_url,data_endpoint))\n",
    "print(data_status_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df356a",
   "metadata": {},
   "source": [
    "Ahora ejecuté la ETL agregándole un sleep de unos segundos para poder ejecutar la misma request y obtener otra respuesta y se obtiene lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b1a9a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Data\":\"Data being ingested since 2023-01-12 22:25:01.964140\",\"Status\":\"200\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_endpoint = \"/data_status\"\n",
    "data_status_response = requests.get(\"{}{}\".format(api_url,data_endpoint))\n",
    "print(data_status_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81a0e8",
   "metadata": {},
   "source": [
    "### 3.  La solución debe ser escalable a 100 millones de entradas. Se recomienda simplificar los datos mediante un modelo de datos. Agregue pruebas de que la solución es escalable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a5ff4b",
   "metadata": {},
   "source": [
    "El código está desarrollado de manera que se lee el csv en chunks de data. (como se muestra en la línea 30). Por lo tanto el código está preparado para leer de a 1 millón de registros a la vez, para no saturar la memoria, por lo que debiese ser escalable a cualquier solución. Quizás si los chunks son muy pequeños el código tarda más, por lo que quizás habría que ir jugando con el tamaño.\n",
    "<br>\n",
    "También pensé en las siguientes opciones que no alcancé a implementar:<br>\n",
    "1. Dentro de cada chunk procesar paralelamente cada región\n",
    "2. El comando MERGE que comenté más arriba también habría acelerado el proceso de incersión"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
